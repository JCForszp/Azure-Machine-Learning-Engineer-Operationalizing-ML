
# Machine Learning Engineer With Microsoft Azure Nanodegree Program / Project 2: operationalizing Machine Learning

## Summary

This project is meant to demo two different processings available on Azure Machine Learning Studio, both relating to the same dataset (a bank marketing data that can be retrieved [here](https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv).  
- The first processing, that will be demoed in the first section of this document, consists in identifying the best prediction model, deploying it in Azure and consuming it from a local python script.   
- The second processing consists in the setup of a complete pipeline totally driven by the Azure Python SDK. The corresponding Jupyter notebook (with the details of the execution steps) can be found in the starter files folder, shortcut [here](https://github.com/JCForszp/nd00333_AZMLND_C2/blob/master/starter_files/aml-pipelines-with-automated-machine-learning-step.ipynb) ).  

The [starter_files](https://github.com/JCForszp/nd00333_AZMLND_C2/tree/master/starter_files) folder, in this repository, contains the list of all final versions of the files (data, scripts, shell files) needed to replicate all steps mentioned below. 


## Architectural Diagram

The structure of this document follows the organization / architecture of the Azure environment. 
As we will go through each individual step of the two processes, it makes sense to keep the same organization to ensure that the transition between every step remaains clear and fluid. 

The overall structure will be hence the following:
![Architecture and document structure](https://user-images.githubusercontent.com/36628203/124170564-e1588300-daa7-11eb-8c44-56fddda19702.png)


## Part 1 - Identification & Deployment of the best predictive model with Azure ML Studio
### Section 1 - Authentication & Dataset registration
#### 1.1 Authentication
#### 1.2 Dataset retrieval & registration
### Section 2 - AutoML - Best Model Identification & deployt
#### 2.1 Create experiment & compute cluster
#### 2.2 Deployment of best model
#### 2.3 Enabling logging
### Section 3 - Accessing and consuming endpoints
#### 3.1 Documenting Endpoints with Swagger
#### 3.2 Consuming deployed services
#### 3.3 benchmarking

## Part 2 - Automating the creation, training and publishing with a pipeline driven from the Azure Python SDK
### 1. Pipeline creation
### 2. Pipeline publishing




## Part 2


*TODO*: Write a short discription of the key steps. Remeber to include all the screencasts required to demonstrate key steps. 

*TODO* Remeber to provide screenshots of the `RunDetails` widget as well as a screenshot of the best model trained with it's parameters.

## Screen Recording
*TODO* Provide a link to a screen recording of the project in action. Remember that the screencast should demonstrate:
[Screen recording](https://youtu.be/M10-TOFYvAM)

## Standout Suggestions
*TODO (Optional):* This is where you can provide information about any standout suggestions that you have attempted.
